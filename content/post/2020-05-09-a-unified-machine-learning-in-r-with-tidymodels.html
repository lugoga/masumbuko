---
title: A Unified Machine Learning in R with tidymodels
author: Masumbuko Semba
date: '2020-05-09'
slug: a-unified-machine-learning-in-r-with-tidymodels
categories:
  - Machine learning
  - Artificial intelligence
tags: 
  - Lake Tanganyika
  - Lates stappersii
  - Stolothrissa tanganicae
  - Limnothrissa miodon
  - Masumbuko Semba
  - tidymodels
  - tidyverse
  - Tanzania
bibliography: [blog.bib]
# csl: apa.csl
link-citations: yes

---



<p><img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/unnamed-chunk-1-1.png" width="192" style="display: block; margin: auto;" /></p>
<div id="tidymodels" class="section level2">
<h2>tidymodels</h2>
<p><strong>tidymodels</strong> is a suite of packages that make machine learning with R a breeze. R has many packages for machine learning, each with their own syntax and function arguments. <strong>tidymodels</strong> aims to provide an unified interface, which allows data scientists to focus on the problem they’re trying to solve, instead of wasting time with learning package syntax.</p>
<p>The tidymodels has a modular approach meaning that specific, smaller packages designed to work hand in hand. Thus, tidymodels is to modeling what the tidyverse is to data wrangling<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. The packages included in <strong>tidymodels</strong> are:</p>
<ul>
<li><strong>parsnip</strong> for model definition <span class="citation">(Kuhn and Vaughan <a href="#ref-parsnip" role="doc-biblioref">2020</a><a href="#ref-parsnip" role="doc-biblioref">a</a>)</span></li>
<li><strong>recipes</strong> for data preprocessing and feature engineering <span class="citation">(Kuhn and Wickham <a href="#ref-recipes" role="doc-biblioref">2020</a><a href="#ref-recipes" role="doc-biblioref">a</a>)</span></li>
<li><strong>rsample</strong> to resample data (useful for cross-validation) <span class="citation">(Kuhn, Chow, and Wickham <a href="#ref-rsample" role="doc-biblioref">2020</a>)</span></li>
<li><strong>yardstick</strong> to evaluate model performance <span class="citation">(Kuhn and Vaughan <a href="#ref-yardstick" role="doc-biblioref">2020</a><a href="#ref-yardstick" role="doc-biblioref">b</a>)</span></li>
<li><strong>dials</strong> to define tuning parameters of your models <span class="citation">(Kuhn <a href="#ref-dials" role="doc-biblioref">2020</a><a href="#ref-dials" role="doc-biblioref">a</a>)</span></li>
<li><strong>tune</strong> for model tuning <span class="citation">(Kuhn <a href="#ref-tune" role="doc-biblioref">2020</a><a href="#ref-tune" role="doc-biblioref">b</a>)</span></li>
<li><strong>workflows</strong> which allows you to bundle everything together and train models easily <span class="citation">(Vaughan <a href="#ref-workflows" role="doc-biblioref">2020</a>)</span></li>
</ul>
<p>In this post, I will walk through a machine learning example from start to end and explain how to use the appropriate <strong>tidymodels</strong> packages at each place. Figure <a href="#fig:fig0">1</a> illustrates key modeling steps that are unified in <strong>tidymodels</strong> that we are going to use use in this article:</p>
<div class="figure"><span id="fig:fig0"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig0-1.png" alt="Conceptual model tidymodel's concept. Courtesy of Edgar Ruiz" width="672" />
<p class="caption">
Figure 1: Conceptual model tidymodel’s concept. Courtesy of Edgar Ruiz
</p>
</div>
<p>After a brief introduction, we proceed with loading some packages we need to work with. We can load the <strong>tidymodels</strong> <span class="citation">(Kuhn and Wickham <a href="#ref-tidymodels" role="doc-biblioref">2020</a><a href="#ref-tidymodels" role="doc-biblioref">b</a>)</span> and <strong>tidyverse</strong> <span class="citation">(Wickham <a href="#ref-tidyverse" role="doc-biblioref">2017</a>)</span> packages into our session. Loading the <strong>tidymodels</strong> package loads a bunch of packages for modeling and also a few others from the tidyverse like ggplot2 and dplyr. You can use use <code>libray</code> function to load the package, but I prefer <code>require</code> for loading the packages in R.</p>
<pre class="r"><code>require(tidymodels)
require(tidyverse)</code></pre>
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<p>We use the <code>dagaa</code> dataset to illustrate the concept. This dataset contains 3,869 observations of three variables: species, total length and weight. Speceis is a factor with three levels describing three species of sardines in Lake Tanganyika. We can import the the file using <code>read_csv</code> function from <strong>readr</strong> package <span class="citation">(Wickham, Hester, and Francois <a href="#ref-readr" role="doc-biblioref">2017</a>)</span>.</p>
<pre class="r"><code>dagaa.clean = read_csv(&quot;dagaa.csv&quot;)</code></pre>
<p>A quick skim of the dataset reveal that there are three variables <span class="citation">(Waring et al. <a href="#ref-skimr" role="doc-biblioref">2020</a>)</span>, a Species factor variable and total length and weight of species as numerical variables without missing values. <strong>tidymodel</strong> prefer string variables as factor and since the Species variable is in factor already, we need no any transformation for now.</p>
<pre class="r"><code>dagaa.clean %&gt;% 
  skimr::skim()</code></pre>
<table>
<caption><span id="tab:unnamed-chunk-4">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">3869</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">3</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">character</td>
<td align="left">1</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">2</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: character</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">min</th>
<th align="right">max</th>
<th align="right">empty</th>
<th align="right">n_unique</th>
<th align="right">whitespace</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">total_length</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">80.50</td>
<td align="right">36.80</td>
<td align="right">33.00</td>
<td align="right">64.00</td>
<td align="right">77.00</td>
<td align="right">85.00</td>
<td align="right">480</td>
<td align="left">▇▁▁▁▁</td>
</tr>
<tr class="even">
<td align="left">weight</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">7.12</td>
<td align="right">27.45</td>
<td align="right">0.09</td>
<td align="right">1.75</td>
<td align="right">2.98</td>
<td align="right">4.06</td>
<td align="right">707</td>
<td align="left">▇▁▁▁▁</td>
</tr>
</tbody>
</table>
<p>Figure <a href="#fig:fig99">2</a> shows unfit between the points and regression lines of the three species. However, the quadratic fits well the data. This indicates that there is a non–linear relation between total length and weight of the three species, and a higher order terms may solve the problem.</p>
<pre class="r"><code>dagaa.clean %&gt;% 
  filter(total_length &lt;= 600)%&gt;%
  ggplot(aes(x = total_length, y = weight))+
  geom_jitter(alpha = .2)+
  geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ poly(x, 2)&quot;, se = FALSE, 
              show.legend = TRUE, aes(color = &quot;Quadratic&quot;)) +
  geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, se = FALSE,
              show.legend = TRUE, aes(color = &quot;Linear&quot;))+
  facet_wrap(~species, scales = &quot;free&quot;)+
  ggpubr::theme_pubclean()+
  scale_color_manual(name = &quot;Model&quot;, values = c(&quot;blue&quot;, &quot;red&quot;))+
  theme(legend.position = &quot;right&quot;, 
        strip.background = element_blank(),
        legend.key = element_blank())+
  labs(x = &quot;Total length (mm)&quot;, y = &quot;Weight of fish (gm)&quot;)</code></pre>
<div class="figure"><span id="fig:fig99"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig99-1.png" alt="Quadratic and linear regression line superimposed in scatterplot with raw data" width="672" />
<p class="caption">
Figure 2: Quadratic and linear regression line superimposed in scatterplot with raw data
</p>
</div>
<p>Figure <a href="#fig:fig98">3</a> indicates that both linear and quadratic models fits well the log–transformed data. This is very clear picture that tell us we need to transform the data before we model them.</p>
<pre class="r"><code>dagaa.clean %&gt;% 
  ggplot(aes(x = total_length, y = weight))+
  scale_y_continuous(trans = log_trans(), labels = function(x) round(x, -2))+
  scale_x_continuous(trans = log_trans(), labels = function(x) round(x, -2))+
  geom_jitter(alpha = .2)+
  geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ poly(x, 2)&quot;, 
              se = FALSE, show.legend = TRUE, aes(color = &quot;Quadratic&quot;)) +
  geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, 
              se = FALSE,show.legend = TRUE, aes(color = &quot;Linear&quot;))+
  facet_wrap(~species)+
  ggpubr::theme_pubclean()+
  scale_color_manual(name = &quot;Model&quot;, values = c(&quot;blue&quot;, &quot;red&quot;))+
  theme(legend.position = &quot;right&quot;, 
        strip.background = element_blank(),
        legend.key = element_blank())+
  labs(x = &quot;Total length (mm)&quot;, y = &quot;Weight of fish (gm)&quot;)</code></pre>
<div class="figure"><span id="fig:fig98"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig98-1.png" alt="Quadratic and linear regression line superimposed in scatterplot with log-transformed data" width="672" />
<p class="caption">
Figure 3: Quadratic and linear regression line superimposed in scatterplot with log-transformed data
</p>
</div>
<p>We can compute the correlation of total length against weight for each species. The result show a strong correlation coefficient of length and weight for the three species.</p>
<pre class="r"><code>dagaa.clean %&gt;% group_by(species) %&gt;%
  summarise(sample = n(), 
            correlation = cor(total_length, weight))</code></pre>
<pre><code># A tibble: 3 x 3
  species sample correlation
  &lt;chr&gt;    &lt;int&gt;       &lt;dbl&gt;
1 Limno     1597       0.934
2 Lstp       128       0.863
3 Stolo     2144       0.946</code></pre>
<p>We can further test whether the relationship is significant with <code>cor.test</code> function. Because this function does not work in group, I have used a <code>for</code> loop to iterate the process and compute the statistic for each species individually and then stitch them with the <code>bind_rows</code> function.</p>
<pre class="r"><code>viumbe = dagaa.clean %&gt;% distinct(species) %&gt;% pull()

cor.stats = list()

for (i in 1:length(viumbe)){

cor.stats[[i]] = dagaa.clean %&gt;%
  filter(species == viumbe[i]) %$%
  cor.test(total_length, weight, method = &quot;pearson&quot;) %&gt;% 
  broom::tidy() %&gt;%
  mutate_if(is.numeric, round, digits = 2,) %&gt;%
  mutate(Species = viumbe[i])%&gt;%
  select(Species, 2,1,5:6,3 )
  
}

cor.stats %&gt;% bind_rows()</code></pre>
<pre><code># A tibble: 3 x 6
  Species statistic estimate conf.low conf.high p.value
  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 Lstp         19.2     0.86     0.81      0.9        0
2 Stolo       134.      0.95     0.94      0.95       0
3 Limno       104.      0.93     0.93      0.94       0</code></pre>
<p>The printed result shows the estimated values (i.e the correlation coefficient), the lower and higher confidence interval and whether the estimated correlation is significant. We see that the length and weight for the three species is strong (R<sup>2</sup> &gt; 0.8) ans is significant (<em>p</em> &lt; 0.05). <em>Stolothrissa</em> species have the higher coefficient (R<sup>2</sup> &gt; 0.95), compared to <em>Limnothrissa</em> (R<sup>2</sup> &gt; 0.93) and <em>Lates</em> (R<sup>2</sup> &gt; 0.8).</p>
</div>
<div id="preprocess" class="section level2">
<h2>Pre–Process</h2>
<p>Data pre–processing is an important stage in modelling, which clean and transform a raw dataset into useful and understandable format. In layman’s terms, raw data is often incomplete, inconsistent, and might be lacking certain behaviors, and is likely to contain many errors, hence that’s when pre-process comes in.</p>
<div id="data-sampling" class="section level3">
<h3>Data sampling</h3>
<p>First, let’s split the dataset into raining and testing set. We will use the training set of iris dataset to train and fit the model and the testing set to evaluate our final model performance. The split is handled automatically using a <code>initial_split</code> function from <strong>rsample</strong> package <span class="citation">(Kuhn, Chow, and Wickham <a href="#ref-rsample" role="doc-biblioref">2020</a>)</span>, which creates a special <code>split object</code>.</p>
<pre class="r"><code>set.seed(234589)

dagaa.split = dagaa.clean %&gt;% 
  rsample::initial_split(prop = .8, strata = &quot;species&quot;)</code></pre>
<p>The printed output of <code>iris.split</code> inform us about the number observation we have sampled as trained and testing set along with the total observations.We can extract training and testing sets from the split object using <code>training</code> and <code>testing</code> functions. At a later stage we will tune parameter in the model and hence we want to use a cross-validation object. We also create a cross–validation object from the training set using the <code>vfold_cv</code> function.</p>
<pre class="r"><code>## train.set
dagaa.train = dagaa.split  %&gt;% 
  rsample::training()

## test set
dagaa.test = dagaa.split %&gt;% 
  rsample::testing()

## cross validation set
iris.cv = dagaa.train %&gt;% 
  rsample::vfold_cv()</code></pre>
</div>
<div id="define-a-recipe" class="section level3">
<h3>Define a recipe</h3>
<p><strong>tidymodels</strong> comes with a <strong>recipes</strong> packages, which provides an interface to specify the role of each variable as either an outcome or predictor using a formula. It also provides pre–processing functions for transforming raw data.</p>
<p>Creating a recipe object with tidymodels package involve two steps chained with pipes. These steps include;</p>
<ol style="list-style-type: decimal">
<li><strong>Specify the formula</strong>—specify the formula that feed the predictor and outcome variables</li>
<li><strong>Specify steps</strong>— you specify data transformation steps. Each data transformation is a step. Functions corresponding to specific types of steps has a prefix <code>step_</code>. <strong>recipes</strong> has several <code>step_*</code> functions.</li>
</ol>
<p>Looking on figure <a href="#fig:fig1">4</a>, we notice that weight and total length of the three species varies.</p>
<pre class="r"><code>dagaa.clean %&gt;% pivot_longer(cols = 2:3, names_to = &quot;variable&quot;, values_to = &quot;data&quot;) %&gt;%
  group_by(species, variable) %&gt;%
  summarise(data.mean = mean(data),
            data.sd = sd(data),
            upper = data.mean+data.sd,
            lower = data.mean-data.sd)%&gt;%
  ggplot(aes(x = variable, y = data.mean, col = species)) +
  geom_point(position = position_dodge(.4), size = 4)+
  geom_errorbar(aes(ymin = lower, ymax = upper),
                position = position_dodge(.4), width = .25)+
  ggsci::scale_color_jco(label = c(&quot;Limnothrisa&quot;,&quot;Lates&quot;, &quot;Stolothrissa&quot;))+
  ggpubr::theme_pubclean()+
  theme(legend.title = element_blank(), legend.key = element_blank(), 
        legend.key.width = unit(2, &quot;lines&quot;), legend.position = &quot;right&quot;)+
  coord_cartesian(expand = TRUE) +
  scale_y_continuous(name = &quot;Morphometric&quot;, breaks = seq(50,350,50))+
  scale_x_discrete(name = &quot;&quot;, labels = c(&quot;Total Length (cm)&quot;, &quot;Weight (gm)&quot;))</code></pre>
<div class="figure" style="text-align: c"><span id="fig:fig1"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig1-1.png" alt="Morphometric measurment of iris flower. The length and width varies for iris flower species" width="576" />
<p class="caption">
Figure 4: Morphometric measurment of iris flower. The length and width varies for iris flower species
</p>
</div>
<p>To model them as predictor of species type, we must transform these numeric variables. First, we use <code>step_corr</code> to check the correlation of predictor variables and if two or more variables correlate, others are dropped and one is retained. Then, use the <code>step_normalize</code> to scale and center total length and weight numeric data to have a standard deviation of one.</p>
<p>Other important feature is that we can apply step to a specific variable, groups or all variables in the dataset. The <code>all_outcomes</code> and <code>all_predictors</code> functions provide convenient ways to specify groups of variables. For instance, if we want <code>step_normalize</code> to transform only predictors we simply parse <code>step_corr(all_predictors())</code>.</p>
<p>After a brief introduction of <em>recipe</em>, we can now go on and create a recipe object. We use the training set and not the raw iris dataset to define the following recipe, transform and prep.</p>
<pre class="r"><code>dagaa.recipe = dagaa.train %&gt;% 
  recipe(species ~ .) %&gt;%
  step_normalize(all_numeric()) %&gt;%
  # step_dummy(sex, stage_mat) %&gt;%
  step_corr(all_numeric()) %&gt;%
  prep()</code></pre>
<p>You notice that I have specified the short formula <code>species ~ .</code>, where <code>.</code> represents all variable in the dataset. If we are interested with the recipe object we just created, we can simply print it in a console.</p>
<pre class="r"><code>dagaa.recipe</code></pre>
<pre><code>Data Recipe

Inputs:

      role #variables
   outcome          1
 predictor          2

Training data contained 3096 data points and no missing data.

Operations:

Centering and scaling for total_length, weight [trained]
Correlation filter removed no terms [trained]</code></pre>
<p>The printed output describes what was done and the how many variables are used as outcome and predictor. It also tells us that the <code>step_corr</code> there is no auto correlation of the variable. We can further extracted the transformed testing dataset from <code>prepped</code> and <code>recipe</code> data data with the <code>juice</code> function. Note that a transformed <code>dagaa.training</code> must originate from the <code>recipe</code>, which is <code>prepped</code>.</p>
<pre class="r"><code>## transformed training set
dagaa.training = dagaa.recipe %&gt;% 
  juice()

dagaa.training %&gt;% glimpse()</code></pre>
<pre><code>Rows: 3,096
Columns: 3
$ total_length &lt;dbl&gt; -0.3398847, -0.2552734, -0.5373111, 5.7521316, 5.07524...
$ weight       &lt;dbl&gt; -0.1851922, -0.1896358, -0.2070400, 5.3789633, 4.34322...
$ species      &lt;fct&gt; Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, ...</code></pre>
<p>We also need to transform the testing set of the dagaa dataset. We can do that by parsing the <code>dagaa.test</code> data set we extracted from the <code>dagaa.split</code> in a <code>bake</code> function. Note that the transformation of the <code>iris.testing</code> must originate from the <code>recipe</code>, which is <code>prepped</code> and <code>recipe</code> dagaa.test` set.</p>
<pre class="r"><code>## transformed testing set
dagaa.testing = dagaa.recipe %&gt;% 
  recipes::bake(dagaa.test)

dagaa.testing %&gt;% glimpse()</code></pre>
<pre><code>Rows: 773
Columns: 3
$ total_length &lt;dbl&gt; 5.188056, 5.780335, 5.244464, 4.257331, 4.652184, 5.27...
$ weight       &lt;dbl&gt; 4.605773, 5.321196, 4.282129, 2.792775, 3.621511, 3.71...
$ species      &lt;fct&gt; Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, Lstp, ...</code></pre>
</div>
</div>
<div id="train-a-model" class="section level2">
<h2>Train a model</h2>
<p>So far we have split the dataset into training and testing, we have create a prepped recipe object, extracted transformed training set and transformed the testing set. The next step we need to do is to train our model using the <strong>parsnip</strong> package <span class="citation">(Kuhn and Vaughan <a href="#ref-parsnip" role="doc-biblioref">2020</a><a href="#ref-parsnip" role="doc-biblioref">a</a>)</span>. <strong>parsnip</strong> provides a unified interface of different models that exist in R.</p>
<p>There are a few primary components that you need to provide for the model specification. These includes;</p>
<ol style="list-style-type: decimal">
<li><strong>Model type</strong> specify a function that define a model like <code>rand_forest</code> for random forest and <code>logistic_reg</code> for logistic regression. If you are wondering of type of modes to use, you can consult <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">this article</a> that describe different function <strong>tidymodels</strong> support</li>
<li><strong>Set arguments</strong> are set using <code>set_args</code></li>
<li><strong>Set engine</strong> is the specific package that the model you choose must come from. For instance the <code>ranger</code> engine drives the Random Forest model and <code>glmnet</code> drives the Logistic regression. You specify the engine to use in the model with the <code>set_engine</code> function.</li>
<li><strong>Set mode</strong> allows to specify the type of prediction—whether <code>classification</code> for binary/categorical prediction or <code>regression</code> for continuous prediction.</li>
</ol>
<p>For instance, if we want to fit a random forest model as implemented by the <code>ranger</code> package for the purpose of classification, the <code>rand_forest()</code> function is used to initialize a Random Forest model and parse the <code>trees</code> argument to define number of tree. Then we use <code>set_engine()</code> function to command rand_forest<code>function for Random Forest model we specified must come from</code>ranger<code>package. Since the prediction is binary, we used the</code>set_mode<code>to specify the type of model. Finally, to execute the model, the</code>fit()` function is used.</p>
<p>This will automatically train the model specified using the transformed training data. Notice that the model runs on top of the juiced trained data because is a transformed instead of the raw trained. The chunk below shows the model specification.</p>
<pre class="r"><code>rf.model = rand_forest() %&gt;%
  set_engine(engine = &quot;ranger&quot;) %&gt;%
  set_mode(mode = &quot;classification&quot;) %&gt;%
  fit(species~., data = dagaa.training)

rf.model</code></pre>
<pre><code>parsnip model object

Fit time:  720ms 
Ranger result

Call:
 ranger::ranger(formula = formula, data = data, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) 

Type:                             Probability estimation 
Number of trees:                  500 
Sample size:                      3096 
Number of independent variables:  2 
Mtry:                             1 
Target node size:                 10 
Variable importance mode:         none 
Splitrule:                        gini 
OOB prediction error (Brier s.):  0.1553012 </code></pre>
<p>Note that the printed fit object provide the model result.</p>
</div>
<div id="metrics-to-evaluate-classification-model" class="section level2">
<h2>Metrics To Evaluate Classification Model</h2>
<p>The <strong>yardstick</strong> package is specifically designed to measure model performance for both numeric and categorical outcomes, and it plays well with grouped predictions. A nice thing about <strong>tidymodels</strong> is that when we use <code>predict</code> function in the fitted model against the transformed testing set, the output is the tibble.</p>
<div id="accuracy-and-kappa" class="section level3">
<h3>Accuracy and Kappa</h3>
<p><code>Accuracy</code> is the percentage of correctly classifies instances out of all instances. It is more useful on a binary classification than multi-class classification problems because it can be less clear exactly how the accuracy breaks down across those classes (e.g. you need to go deeper with a confusion matrix). Learn more about Accuracy here.</p>
<p><code>Kappa</code> or <code>Cohen’s Kappa</code> is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0).</p>
<p>since all the prediction information is tibble, we can apply the <code>metric</code> function from <strong>yardstick</strong> and compare original species value (<code>truth = Species</code>) against predicted species values (<code>estimate = .pred_class</code>) to evaluate our model performance. The function expects a tibble that contains the actual results (truth) and what the model predicted (estimate). The <code>metrics()</code> function calculates <code>accuracy</code> and <code>kap</code> metrics for numeric outcomes. Furthermore, it automatically recognizes that <code>lm_preds</code> is grouped by folds and thus calculates the metrics for each fold.</p>
<pre class="r"><code>rf.model %&gt;% predict(dagaa.testing) %&gt;%
  bind_cols(dagaa.testing) %&gt;%
  metrics(truth = species, estimate = .pred_class)</code></pre>
<pre><code># A tibble: 2 x 3
  .metric  .estimator .estimate
  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
1 accuracy multiclass     0.771
2 kap      multiclass     0.570</code></pre>
<p>The printed output indicates that our model has an accuracy of 0.77 andd the kappa of 0.56.</p>
</div>
<div id="gain-and-roc-curve" class="section level3">
<h3>Gain and Roc Curve</h3>
<p>When we develop statistical models for classification tasks (e.g. using machine learning), we usually need to have a way to compare the generated models results to help us decide whether the model fitted well the data—the best model. Some of the tools in <strong>yardstick</strong> package include <code>gains</code> and `roc1 tools.</p>
<p>The Gains and the ROC curve are visualizations showing overall performance of the models. The shape of the curves will tell us a lot about the behavior of the model. It clearly shows how much our model is better than a model assigning categories randomly and how far we are from the optimal model which is in practice unachievable. These curves can help in setting the final cut-off point for deciding which probabilities will mean positive and negative response prediction.</p>
<p>In order to plot gain or roc curve, we need first to compute the probability for each possible predicted value by setting the type argument to <code>prob.</code> That will return a tibble with as many variables as there are possible predicted values. Their name will default to the original value name, prefixed with <code>.pred_</code>.</p>
<pre class="r"><code>dagaa.prob = rf.model %&gt;% 
  predict(dagaa.testing, type = &quot;prob&quot;) %&gt;%
  bind_cols(dagaa.testing)</code></pre>
</div>
<div id="gain-curve" class="section level3">
<h3>Gain Curve</h3>
<p>Once we have the probability of prediction, we can use <code>gain_curve</code> function fro <strong>yardstick</strong> package to compute values for gain curve. Figure <a href="#fig:fig4">5</a> is a gain plot. The gains associated with the model is shown as black curve for the three species.</p>
<pre class="r"><code>dagaa.gain = dagaa.prob %&gt;%
  yardstick::gain_curve(species, 
                        .pred_Limno, 
                        .pred_Lstp, 
                        .pred_Stolo)

dagaa.gain %&gt;% 
  autoplot()+
  ggpubr::theme_pubclean()+
  theme(strip.background = element_blank())</code></pre>
<div class="figure"><span id="fig:fig4"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig4-1.png" alt="Gain curve of modelled species" width="672" />
<p class="caption">
Figure 5: Gain curve of modelled species
</p>
</div>
</div>
</div>
<div id="roc-curve" class="section level2">
<h2>ROC curve</h2>
<p>The other tool used to assess the model accuracy based on a confusion matrix are Sensitivity and Specificity. The ROC curve (Receiver Operating Characteristics curve) is the display of <code>sensitivity</code> and <code>specificity</code> for different cut-off values for probability (If the probability of positive response is above the cut-off, we predict a positive outcome, if not we are predicting a negative one). Each cut-off value defines one point on ROC curve, ranging cut-off from 0 to 1 will draw the whole ROC curve. to obtain the value for roc curve, we use the <code>roc_curve</code> function from yardstick.</p>
<pre class="r"><code>dagaa.roc = dagaa.prob %&gt;%
  yardstick::roc_curve(species, 
                        .pred_Limno, 
                        .pred_Lstp, 
                        .pred_Stolo)</code></pre>
<p>The black curve on ROC curve in figure <a href="#fig:fig5">6</a> is the same model as the example for the Gains chart (Figure <a href="#fig:fig4">5</a>). The Y axis measures the rate (as a percentage) of correctly predicted species with a positive response. The X axis measures the rate of incorrectly predicted species with a negative response. Since the optimal model should have sensitivity that rise to a maximum and specificity will stay the whole time at 1. The task is to have ROC curve of the developed model as close as possible to optimal model. Therefore, based on that information, figure <a href="#fig:fig5">6</a> indicates that our model predicted well for <code>Lates</code> species as compared to <code>Limno</code> and <code>Stolo</code>.</p>
<pre class="r"><code>dagaa.roc %&gt;% 
  autoplot()+
  ggpubr::theme_pubclean()+
  theme(strip.background = element_blank())</code></pre>
<div class="figure"><span id="fig:fig5"></span>
<img src="/post/2020-05-09-a-unified-machine-learning-in-r-with-tidymodels_files/figure-html/fig5-1.png" alt="Roc curve of modelled species" width="672" />
<p class="caption">
Figure 6: Roc curve of modelled species
</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>I hanged on <a href="http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/#tune-the-parameters">Rebecca Barter</a> post on <em>Tidymodels: tidy machine learning in R</em> that explain briefly the use of tidymodels package in R. I also glimpsed <a href="https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/">Edger Ruiz</a> post that provide a gentle introduction to tidymodels. These post provided resourceful material for learning basic functions of tidymodels.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-dials">
<p>Kuhn, Max. 2020a. <em>Dials: Tools for Creating Tuning Parameter Values</em>. <a href="https://CRAN.R-project.org/package=dials">https://CRAN.R-project.org/package=dials</a>.</p>
</div>
<div id="ref-tune">
<p>———. 2020b. <em>Tune: Tidy Tuning Tools</em>. <a href="https://CRAN.R-project.org/package=tune">https://CRAN.R-project.org/package=tune</a>.</p>
</div>
<div id="ref-rsample">
<p>Kuhn, Max, Fanny Chow, and Hadley Wickham. 2020. <em>Rsample: General Resampling Infrastructure</em>. <a href="https://CRAN.R-project.org/package=rsample">https://CRAN.R-project.org/package=rsample</a>.</p>
</div>
<div id="ref-parsnip">
<p>Kuhn, Max, and Davis Vaughan. 2020a. <em>Parsnip: A Common Api to Modeling and Analysis Functions</em>. <a href="https://CRAN.R-project.org/package=parsnip">https://CRAN.R-project.org/package=parsnip</a>.</p>
</div>
<div id="ref-yardstick">
<p>———. 2020b. <em>Yardstick: Tidy Characterizations of Model Performance</em>. <a href="https://CRAN.R-project.org/package=yardstick">https://CRAN.R-project.org/package=yardstick</a>.</p>
</div>
<div id="ref-recipes">
<p>Kuhn, Max, and Hadley Wickham. 2020a. <em>Recipes: Preprocessing Tools to Create Design Matrices</em>. <a href="https://CRAN.R-project.org/package=recipes">https://CRAN.R-project.org/package=recipes</a>.</p>
</div>
<div id="ref-tidymodels">
<p>———. 2020b. <em>Tidymodels: Easily Install and Load the ’Tidymodels’ Packages</em>. <a href="https://CRAN.R-project.org/package=tidymodels">https://CRAN.R-project.org/package=tidymodels</a>.</p>
</div>
<div id="ref-workflows">
<p>Vaughan, Davis. 2020. <em>Workflows: Modeling Workflows</em>. <a href="https://CRAN.R-project.org/package=workflows">https://CRAN.R-project.org/package=workflows</a>.</p>
</div>
<div id="ref-skimr">
<p>Waring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2020. <em>Skimr: Compact and Flexible Summaries of Data</em>. <a href="https://CRAN.R-project.org/package=skimr">https://CRAN.R-project.org/package=skimr</a>.</p>
</div>
<div id="ref-tidyverse">
<p>Wickham, Hadley. 2017. <em>Tidyverse: Easily Install and Load the ’Tidyverse’</em>. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>.</p>
</div>
<div id="ref-readr">
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2017. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://hansjoerg.me/2020/02/09/tidymodels-for-machine-learning/">Tutorial on tidymodels for Machine Learning</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
